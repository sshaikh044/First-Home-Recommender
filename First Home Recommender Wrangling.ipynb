{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Home Recommender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "import matplotlib\n",
    "import warnings\n",
    "import time\n",
    "import pickle\n",
    "import yellowbrick as yb\n",
    "import psycopg2 as pg\n",
    "from sqlalchemy import create_engine\n",
    "from functools import reduce\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC, NuSVC, SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from yellowbrick.classifier import ClassificationReport\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to the Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect( \n",
    "                        host = 'project.cgxhdwn5zb5t.us-east-1.rds.amazonaws.com',\n",
    "                        port = 5432, \n",
    "                        user = 'postgres',\n",
    "                        password = 'Admin123',\n",
    "                        database = 'postgres')\n",
    "\n",
    "cursor = conn.cursor()\n",
    "DEC2FLOAT = psycopg2.extensions.new_type(psycopg2.extensions.DECIMAL.values,\n",
    "                                        'DEC2FLOAT',\n",
    "                                         lambda value, curs: float(value) if value is not None else None)\n",
    "psycopg2.extensions.register_type(DEC2FLOAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(ds):\n",
    "    cursor.execute('Select * from {}'.format(ds))\n",
    "    rows = cursor.fetchall()\n",
    "    col_names = []\n",
    "    for elt in cursor.description:\n",
    "        col_names.append(elt[0])\n",
    "    return pd.DataFrame(data=rows, columns=col_names )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "InFailedSqlTransaction",
     "evalue": "current transaction is aborted, commands ignored until end of transaction block\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInFailedSqlTransaction\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-85c6bb5cf7f7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhousehold_2017\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'household_2017'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-21-761fda9d51e5>\u001b[0m in \u001b[0;36mcreate_dataset\u001b[1;34m(ds)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcreate_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mcursor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Select * from {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcursor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetchall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mcol_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0melt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcursor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescription\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInFailedSqlTransaction\u001b[0m: current transaction is aborted, commands ignored until end of transaction block\n"
     ]
    }
   ],
   "source": [
    "household_2017 = create_dataset('household_2017')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_int(df):\n",
    "    cols = list(df.columns)\n",
    "    for col in cols:\n",
    "        if df[col].dtype == object:\n",
    "            df[col] = df[col].str.strip(\"'\").astype('int64')\n",
    "        else:\n",
    "            pass\n",
    "    print(df.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_int(household_2017)\n",
    "convert_to_int(mortgage_2017)\n",
    "convert_to_int(person_2017)\n",
    "convert_to_int(project_2017)\n",
    "convert_to_int(household_2015)\n",
    "convert_to_int(mortgage_2015)\n",
    "convert_to_int(person_2015)\n",
    "convert_to_int(project_2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Year Value\n",
    "household_2017['YEAR'] = 2017\n",
    "mortgage_2017['YEAR'] = 2017\n",
    "person_2017['YEAR'] = 2017\n",
    "project_2017['YEAR'] = 2017\n",
    "household_2015['YEAR'] = 2015\n",
    "mortgage_2015['YEAR'] = 2015\n",
    "person_2015['YEAR'] = 2015\n",
    "project_2015['YEAR'] = 2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate Household, Mortgage, Person, and Project Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_datasets(df1, df2):\n",
    "    return pd.concat([df1, df2], join='inner', ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "household_combined = pd.concat([household_2017, household_2015], join='inner', ignore_index=True)\n",
    "mortgage_combined = pd.concat([mortgage_2017, mortgage_2015], join='inner', ignore_index=True)\n",
    "person_combined = pd.concat([person_2017, person_2015], join='inner', ignore_index=True)\n",
    "project_combined = pd.concat([project_2017, project_2015], join='inner', ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Clean Datasets to PostgreSQL Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tables = {\n",
    "             'household_2017':household_2017, \n",
    "             'mortgage_2017':mortgage_2017, \n",
    "             'person_2017':person_2017,\n",
    "             'project_2017':project_2017,\n",
    "             'household_2015':household_2015, \n",
    "             'mortgage_2015':mortgage_2015, \n",
    "             'person_2015':person_2015,\n",
    "             'project_2015':project_2015,\n",
    "             'household_combined':household_combined,\n",
    "             'mortgage_combined':mortgage_combined,\n",
    "             'person_combined':person_combined,\n",
    "             'project_combined':project_combined\n",
    "            }\n",
    "engine = create_engine('postgresql://postgres:Admin123@project.cgxhdwn5zb5t.us-east-1.rds.amazonaws.com:5432/postgres')\n",
    "\n",
    "for name, df in df_tables.items():\n",
    "    df.to_sql('{}'.format(name), engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_threshold = 0.20\n",
    "\n",
    "\n",
    "cursor.execute('Select * from household_combined')\n",
    "rows = cursor.fetchall()\n",
    "col_names = []\n",
    "for elt in cursor.description:\n",
    "    col_names.append(elt[0])\n",
    "df = pd.DataFrame(data=rows, columns=col_names )\n",
    "\n",
    "varcon = pd.read_csv(os.path.join(os.getcwd(), 'data', 'concordance', 'variable_concordance.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subset the dataset to only first-time home buyers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fh = df[df['FIRSTHOME']==1].copy()\n",
    "df_fh.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create lists of each variable type "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vars = varcon.iloc[:,0]\n",
    "weights_and_flags = varcon[(varcon['Weight']==True) | (varcon['Flag']==True)]['Variable'].values\n",
    "non_exp = varcon[varcon['Not Experience']==True]['Variable'].values\n",
    "target_vars = list(varcon[varcon['Type']=='Target']['Variable'].values)\n",
    "cont_vars = list(varcon[varcon['Type']=='Continuous']['Variable'].values)\n",
    "cat_vars = list(varcon[varcon['Type']=='Categorical']['Variable'].values)\n",
    "binary_vars = list(varcon[varcon['Type']=='Binary']['Variable'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seperate dataset into target variable and dependent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fh2 = df_fh[list(set(all_vars).difference(set(weights_and_flags)))].copy()\n",
    "df_fh2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove variables whose portion of missing values is above the threshhold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.replace([-9], np.nan, inplace=True)\n",
    "\n",
    "miss_percent = df_fh2[cont_vars].isin([-9,-6]).sum(axis=0) / df_fh2[cont_vars].count(axis=0)\n",
    "miss_percent_lt_thresh = miss_percent[miss_percent.iloc[:] < threshold]\n",
    "estimators_cont = df_fh2[['CONTROL','YEAR'] + list(miss_percent_lt_thresh.index)].copy()\n",
    "estimators_cont.replace([-9,-6], np.nan, inplace=True)\n",
    "\n",
    "miss_percent = df_fh2[cat_vars].isin([-9]).sum(axis=0) / df_fh2[cat_vars].count(axis=0)\n",
    "miss_percent_lt_thresh = miss_percent[miss_percent.iloc[:] < threshold]\n",
    "estimators_cat = df_fh2[['CONTROL','YEAR'] + list(miss_percent_lt_thresh.index)].copy()\n",
    "estimators_cat.replace([-9], np.nan, inplace=True)\n",
    "\n",
    "miss_percent = df_fh2[binary_vars].isin([-9,-6]).sum(axis=0) / df_fh2[binary_vars].count(axis=0)\n",
    "miss_percent_lt_thresh = miss_percent[miss_percent.iloc[:] < threshold]\n",
    "estimators_binary = df_fh2[['CONTROL','YEAR'] + list(miss_percent_lt_thresh.index)].copy()\n",
    "estimators_binary.replace([-9,-6], np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep only variales that capture \"housing experience\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [target, estimators_cont, estimators_cat, estimators_binary]\n",
    "df_fh_all_vars = reduce(lambda left, right: pd.merge(left, right, how='inner', \n",
    "                                                   on=['CONTROL','YEAR']), dfs).dropna(how='any')\n",
    "\n",
    "cont_vars = list(set(estimators_cont.columns) & set(non_exp))\n",
    "cat_vars = list(set(estimators_cat.columns) & set(non_exp))\n",
    "binary_vars = list(set(estimators_binary.columns) & set(non_exp))\n",
    "\n",
    "estimators_cont = estimators_cont[['CONTROL','YEAR'] + list(set(estimators_cont.columns) & set(non_exp))]\n",
    "estimators_cat = estimators_cat[['CONTROL','YEAR'] + list(set(estimators_cat.columns) & set(non_exp))]\n",
    "estimators_binary = estimators_binary[['CONTROL','YEAR'] + list(set(estimators_binary.columns) & set(non_exp))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impute missing values for all estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Variables\n",
    "imputer_target = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "imputer_target.fit(target)\n",
    "imputed_target = imputer_target.transform(target)\n",
    "target = pd.DataFrame(imputed_target, columns=target.columns)\n",
    "\n",
    "# Continuous Variables\n",
    "imputer_cont = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "imputer_cont.fit(estimators_cont)\n",
    "imputed_cont = imputer_cont.transform(estimators_cont)\n",
    "estimators_cont = pd.DataFrame(imputed_cont, columns=estimators_cont.columns)\n",
    "\n",
    "# Categorical Variables\n",
    "imputer_cat = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "imputer_cat.fit(estimators_cat)\n",
    "imputed_cat = imputer_cat.transform(estimators_cat)\n",
    "estimators_cat = pd.DataFrame(imputed_cat, columns=estimators_cat.columns)\n",
    "\n",
    "#Binary Variables\n",
    "imputer_binary = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "imputer_binary.fit(estimators_binary)\n",
    "imputed_binary = imputer_binary.transform(estimators_binary)\n",
    "estimators_binary = pd.DataFrame(imputed_binary, columns=estimators_binary.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Datasets Back Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_reg = [target, estimators_cont, estimators_cat_dum, estimators_binary_dum]\n",
    "dfs_class = [target, estimators_cont, estimators_cat, estimators_binary]\n",
    "df_final_reg = reduce(lambda left, right: pd.merge(left, right, how='inner', on=['CONTROL','YEAR']), dfs_reg).dropna(how='any')\n",
    "df_final_class = reduce(lambda left, right: pd.merge(left, right, how='inner', on=['CONTROL','YEAR']), dfs_class).dropna(how='any')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Transformations\n",
    "\n",
    "### Take the Natural Log of Income Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_reg['LN_HINCP'] = np.where(df_final_reg['HINCP'] > 1, np.log(df_final_reg['HINCP']), 0)\n",
    "df_final_reg['LN_FINCP'] = np.where(df_final_reg['FINCP'] > 1, np.log(df_final_reg['FINCP']), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a floor and ceiling to the income variables and put them in 10 bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_class['HINCP_BIN'] = np.where(df_final_class['HINCP']>100000, 100000, df_final_class['HINCP'])\n",
    "df_final_class['FINCP_BIN'] = np.where(df_final_class['FINCP']>100000, 100000, df_final_class['FINCP'])\n",
    "df_final_class['HINCP_BIN'] = np.where(df_final_class['HINCP']<=0, 0, df_final_class['HINCP'])\n",
    "df_final_class['FINCP_BIN'] = np.where(df_final_class['FINCP']<=0, 0, df_final_class['FINCP'])\n",
    "df_final_class['HINCP_BIN'] = pd.cut(df_final_class['HINCP'], bins=np.linspace(0,100000,11), include_lowest=True)\n",
    "df_final_class['FINCP_BIN'] = pd.cut(df_final_class['FINCP'], bins=np.linspace(0,100000,11), include_lowest=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Transformed Data to PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tables = { 'ahs_household_class':df_final_class,\n",
    "             'ahs_household_reg':df_final_reg}\n",
    "engine = create_engine('postgresql://postgres:Admin123@project.cgxhdwn5zb5t.us-east-1.rds.amazonaws.com:5432/postgres')\n",
    "\n",
    "for name, df in df_tables.items():\n",
    "    df.to_sql('{}'.format(name), engine)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
